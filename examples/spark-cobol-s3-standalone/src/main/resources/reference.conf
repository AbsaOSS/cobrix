
hadoop.redacted.tokens = [password, secret, session.token]

hadoop.option {
  # Authentication providerc can be
  #  - org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
  #  - com.amazonaws.auth.profile.ProfileCredentialsProvider
  #  - com.amazonaws.auth.InstanceProfileCredentialsProvider
  # Use the default provider chain. It will use the first authentication provider that succeeds
  #fs.s3a.aws.credentials.provider = "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
  fs.s3a.aws.credentials.provider = "com.amazonaws.auth.profile.ProfileCredentialsProvider"

  # AWS credentials
  #fs.s3a.access.key = ""
  #fs.s3a.secret.key = ""
  # The session token for temporary credentials spec
  #fs.s3a.session.token = ""

  # Explicitly specify the endpoint
  #fs.s3a.endpoint = "s3.af-south-1.amazonaws.com"

  # Enable bucket key encruption
  fs.s3a.server-side-encryption-bucket-key-enabled = "true"

  # Specify the KSM key for server-side encryption
  #fs.s3a.server-side-encryption.key = "arn:aws:kms:***"
  #fs.s3a.server-side-encryption-algorithm = "SSE-KMS"
}

# Spark configuration
spark.conf.option {
  # Run Spark in local master mode
  spark.master = "local[*]"

  # You can uncomment these options if you are running under a VPN
  #spark.ui.enabled = "false"
  #spark.driver.bindAddress = "127.0.0.1"
  #spark.driver.host = "127.0.0.1"
}
